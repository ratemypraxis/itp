---
layout: post
title: Final Ideas [ML for the Web]
description: 
summary: 
tags: ml-for-web
---

<h1> Final Project Ideas </h1>
<h2> Idea 1: What Time Is It There? </h2>
A speech-controlled clock and installation which makes use of the <a href="https://worldtimeapi.org/">World Time API</a> to allow users to recieve what time it is in any part of the world. In the installation concext the piece is situated in a room outfitted with smart RGB lightbulbs and a speaker system which outputs abstract simulations of the feel of the given time.
![](https://raw.githubusercontent.com/ratemypraxis/itp/master/media/timeSketch.jpg)

Materials:
+ Stepper Motor(s)
+ Arduino Nano 33 IOT
+ Laser cut wood (clock fabrication) 
+ World Time API
+ p5.Speech or other speech recognition technology (<a href="https://store-usa.arduino.cc/products/speech-recognition-engine">Arduino Speech Recognition Engine</a>?)
+ Phillips Hue (maybe)
+ Bluetooth speakers (maybe)

<a href="https://file.notion.so/f/s/89b7ea76-afd2-4293-9ffa-bfb2aa7f0bcb/servoSpeech1.mp4?id=307b73da-9352-4654-856e-4c799a65b1f4&table=block&spaceId=ea1b9eba-3e00-4e51-a314-65ee5a018a42&expirationTimestamp=1682033005111&signature=QwlDsuu3vWdoxN0s9fDiMPwcIblHob1WxFD3RfRaY1I&downloadName=servoSpeech1.mp4">Link to a video</a> of an aarlier prototyping of similar technology (Voice 2 Servo).


<h1> Idea 2: Grey Area Intelligence System</h1>

An idea for the final for both Machine Learning for the Web and Live Web.

Based on a previous assignment for ML4TW (<a href="https://www.2nd.systems/itp/projects/teachingEarth">linked here</a>) in which a lone user is prompted to provide images of specific emotional phenomena (or otherwise gray area identifications of the physical world) through webcam input making use of ml5.js classification tools. In the final adaptation of this idea I'd like to make use of websockets and a storage system like firebase to allow multiple users to contribute images to each category and build a collectivley trained gray area image recognition algorithm. 

![screencap of a webpage featuring webcam feed of a person appearing frightened.](https://raw.githubusercontent.com/ratemypraxis/itp/master/media/training.jpg)
![](https://raw.githubusercontent.com/ratemypraxis/itp/master/media/trainingSwag.png)

I can see this project taking the form of an installation in which users can choose to train different gray area attitudes and ideas on a small desktop screen and then walk by a larger projection which is putting the training to work by identifying people and assigning pre-taught gray area values to them in a bounding box style based on their facial expressions.

Below are screencaps of a short film I made with a simulated version of a similar idea:

![](https://raw.githubusercontent.com/ratemypraxis/itp/master/media/ga1.png)
![](https://raw.githubusercontent.com/ratemypraxis/itp/master/media/ga2.png)
